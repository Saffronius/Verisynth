# Verifying LLM Generated Policies

This repository contains the experimental code and data accompanying our research paper on verifying LLM-generated policies.

## Overview

The experiments in this repository evaluate various large language models' performance on policy verification tasks, including:

- Explanation consistency
- Request comprehension  
- Semantic equivalence

## Repository Structure

- `baseline/` - Baseline comparison experiments and visualization scripts
- `CPCA/` - CPCA-related experimental code
- `ABC/` - ABC framework experiments
- `*.py` - Various analysis and data processing scripts
- `*.sh` - Experiment execution scripts

## Usage

This code is provided for reproducibility and further research. Please refer to the accompanying paper for detailed methodology and results.

## Citation

If you use this code or data in your research, please cite our paper:

```
[Citation will be added upon publication]
``` 